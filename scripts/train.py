"""
Minimal training loop for CADRL.

This module exposes `run_training(cfg, ...)` which can be imported and called
from `main.py` (CLI handling should live in main.py as requested).

The function implements a simple TD(0) training loop using the project"s
`CADRLEnv`, `ActionSpace`, `ValueNetwork`, `ReplayMemory`, and `CADRLAgent`.
"""
from typing import Optional, Dict, List
import os
import time

import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm

from core import ConfigContainer, get_config_container
from env import CADRLEnv
from agent import CADRLAgent
from models import ValueNetwork
from utils import ActionSpace, ReplayMemory
from utils.trajectory import Trajectory
from utils.value_target import ArrivalTimeTarget, TDTarget


def load_trajectories(path: str, *, skip_timed_out: bool = True) -> List[Trajectory]:
    """
    Load trajectories from `path` and return a list of `Trajectory` objects.

    NOTE: placeholder. Implement data loading in your environment and
    return a List[Trajectory].
    """
    trajs: List[Trajectory] = []
    if not os.path.exists(path):
        return trajs

    files = sorted([f for f in os.listdir(path) if f.endswith(".npz")])
    for fn in files:
        fp = os.path.join(path, fn)
        try:
            with np.load(fp) as data:
                # skip trajectories that were timed out or went out of bounds (generator may add these fields)
                if skip_timed_out and (bool(data.get("timed_out", False)) or bool(data.get("out_of_bounds", False))):
                    continue
                times = data["times"]
                positions = data["positions"]  # (T,2,2)
                v_pref = float(data.get("v_pref", 1.0))
                radius = float(data.get("radius", 0.3))
                goal_x = float(data.get("goal_x", 0.0))
                goal_y = float(data.get("goal_y", 0.0))
                kinematic = bool(data.get("kinematic", True))

                # build Trajectory: requires gamma, goal_x, goal_y, radius, v_pref, times, positions
                traj = Trajectory(
                    gamma=float(getattr(get_config_container().model, "gamma", 0.8)),
                    goal_x=float(goal_x),
                    goal_y=float(goal_y),
                    radius=float(radius),
                    v_pref=float(v_pref),
                    times=np.asarray(times, dtype=np.float32),
                    positions=np.asarray(positions, dtype=np.float32),
                    kinematic=bool(kinematic),
                )
                trajs.append(traj)
        except Exception: # pylint: disable=broad-except
            # skip malformed files
            continue
    return trajs


def _make_dirs(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def _save_checkpoint(model: nn.Module, path: str) -> None:
    torch.save({"state_dict": model.state_dict()}, path)


def run_training(
    cfg: Optional[Dict] = None,
    *,
    device: Optional[torch.device] = None,
    resume: Optional[str] = None,
    run_name: Optional[str] = None,
) -> None:
    """
    Run a minimal TD training loop.

    Parameters
    ----------
    cfg : Optional[Dict]
        If None, load configuration via `get_config_container()`; otherwise
        expects a ConfigContainer or dict-like mapping with required fields.
    device : Optional[torch.device]
        Device to run training on; if None, use cfg.device.
    resume : Optional[str]
        Path to a checkpoint to resume from (optional).
    run_name : Optional[str]
        Name for this run; used to create checkpoint/log directories.

    Notes
    -----
    This is intentionally minimal: it demonstrates integration with the
    repository"s components. It is not optimized for speed or stability.
    """
    # load config container if needed
    if cfg is None:
        cfg = get_config_container()
    # allow passing a raw dict -> get a ConfigContainer
    if isinstance(cfg, dict):
        cfg = ConfigContainer.from_dict(cfg)
    if not hasattr(cfg, "agent"):
        # fallback to loader
        cfg = get_config_container()

    device = device or getattr(cfg, "device", torch.device("cpu"))

    # build paths
    base_dir = os.path.join("checkpoints", run_name or time.strftime("%Y%m%d-%H%M%S"))
    _make_dirs(base_dir)

    # environment and action space
    env = CADRLEnv(cfg, phase="train")
    action_space = ActionSpace(cfg.agent.v_pref, cfg.agent.kinematic)

    # model and replay
    model = ValueNetwork(state_dim=cfg.model.state_dim, device=device).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=float(cfg.train.learning_rate))
    replay = ReplayMemory(capacity=int(cfg.train.capacity), state_dim=int(cfg.model.state_dim))

    # -------------------- supervised pretraining (optional) --------------------

    pretrain_epochs = getattr(cfg.init, "pretrain_epochs", 0)
    pretrain_batch_size = getattr(cfg.init, "pretrain_batch_size", 500)
    pretrain_total_iters = getattr(cfg.init, "pretrain_total_iters", 10000)
    traj_dir = getattr(cfg.init, "traj_dir", None)
    if pretrain_epochs > 0 and traj_dir:
        print("\n" + "="*70)
        print("ğŸš€ å¼€å§‹é¢„è®­ç»ƒé˜¶æ®µ (Supervised Pretraining)")
        print("="*70)
        try:
            trajectories = load_trajectories(traj_dir)
        except NotImplementedError:
            trajectories = []

        if trajectories:
            print(f"âœ“ åŠ è½½äº† {len(trajectories)} æ¡è½¨è¿¹æ•°æ®")
            pair_gen = ArrivalTimeTarget(gamma=float(cfg.model.gamma))
            # simple pretraining loop (MSE on arrival time targets)
            preoptimizer = torch.optim.Adam(model.parameters(), lr=float(cfg.train.learning_rate))
            model.train()

            # é¢„è®­ç»ƒè¿›åº¦æ¡
            # å…¼å®¹åŸè®ºæ–‡ï¼šæ€»å…± pretrain_total_iters æ¬¡ï¼Œæ¯æ¬¡ batch_size
            pairs = []
            for traj in trajectories:
                pairs.extend(pair_gen.compute_pairs(traj))
            total_pairs = len(pairs)
            pretrain_pbar = tqdm(range(pretrain_total_iters), desc="é¢„è®­ç»ƒè¿›åº¦",
                                unit="iter", colour="blue", ncols=100)
            for it in pretrain_pbar:
                # éšæœºé‡‡æ · batch
                idx = torch.randperm(total_pairs)[:pretrain_batch_size]
                batch_pairs = [pairs[i] for i in idx]
                s_batch = torch.stack([p[0].flatten() for p in batch_pairs]).to(device)
                y_batch = torch.stack([p[1].flatten() for p in batch_pairs]).to(device)
                pred = model(s_batch)
                loss_pre = nn.functional.mse_loss(pred, y_batch)
                preoptimizer.zero_grad()
                loss_pre.backward()
                preoptimizer.step()
                pretrain_pbar.set_postfix({
                    "loss": f"{loss_pre.item():.4f}",
                    "pairs": total_pairs,
                    "batch": pretrain_batch_size
                })
            pretrain_pbar.close()
            print(f"âœ“ é¢„è®­ç»ƒå®Œæˆï¼å…±è®­ç»ƒ {pretrain_total_iters} æ¬¡ batch")
            # end pretraining
        else:
            print("âš  è­¦å‘Šï¼šæœªæ‰¾åˆ°æœ‰æ•ˆçš„è½¨è¿¹æ•°æ®ï¼Œè·³è¿‡é¢„è®­ç»ƒ")
    else:
        if pretrain_epochs == 0:
            print("\nğŸ“ é¢„è®­ç»ƒå·²ç¦ç”¨ (pretrain_epochs=0)")
        else:
            print("\nâš  è­¦å‘Šï¼šæœªé…ç½®è½¨è¿¹ç›®å½•ï¼Œè·³è¿‡é¢„è®­ç»ƒ")

    # agent that uses the value network for action selection
    agent = CADRLAgent(value_net=model, action_space=action_space, device=device, gamma=float(cfg.model.gamma))

    # optionally resume
    if resume is not None and os.path.exists(resume):
        ckpt = torch.load(resume, map_location=device)
        try:
            model.load_state_dict(ckpt.get("state_dict", ckpt))
        except Exception: # pylint: disable=broad-except
            model.load_state_dict(ckpt)

    # training loop (minimal)
    num_epochs = int(cfg.train.num_epochs)
    sample_episodes = int(cfg.train.sample_episodes)
    batch_size = int(cfg.train.batch_size)

    generator = torch.Generator()

    print("\n" + "="*70)
    print("ğŸ¯ å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒé˜¶æ®µ (Reinforcement Learning)")
    print("="*70)
    print("é…ç½®ä¿¡æ¯:")
    print(f"  - è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"  - æ¯è½®é‡‡æ ·episodes: {sample_episodes}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  - å­¦ä¹ ç‡: {cfg.train.learning_rate}")
    print(f"  - Gamma: {cfg.model.gamma}")
    print(f"  - è®¾å¤‡: {device}")
    print(f"  - æ£€æŸ¥ç‚¹é—´éš”: {cfg.train.checkpoint_interval} epochs")
    print("="*70 + "\n")

    # è®­ç»ƒä¸»å¾ªç¯è¿›åº¦æ¡
    train_pbar = tqdm(range(num_epochs), desc="è®­ç»ƒè¿›åº¦",
                     unit="epoch", colour="green", ncols=100)



    recent_episodes = []  # ç¼“å­˜æœ€è¿‘5æ¡episodeçš„Trajectoryå¯¹è±¡

    for epoch in train_pbar:
        # collect some episodes into replay
        episode_rewards = []
        episode_lengths = []

        for _ in range(sample_episodes):
            states = env.reset()
            done = [False, False]
            steps = 0
            ep_reward = 0.0
            ep_buf = []
            
            # è®°å½•å®Œæ•´è½¨è¿¹ä¿¡æ¯ï¼ˆç”¨äºæ„é€ Trajectoryï¼‰
            ep_times = [0.0]
            ep_positions = [[
                [states[0].self_state.px, states[0].self_state.py],
                [states[0].neighbor_state.px, states[0].neighbor_state.py]
            ]]
            t_acc = 0.0


            def _eps(epoch, total, eps_start=0.2, eps_end=0.02):
                t = min(1.0, epoch / max(1, total))
                return eps_start + (eps_end - eps_start) * t

            while not all(done) and steps < env.max_steps:
                actions = []
                for i in range(2):
                    s = states[i]
                    if torch.rand(()) < _eps(epoch, num_epochs):
                        a = action_space.sample()
                    else:
                        a = agent.act(s, env, i, mode="train")
                    actions.append(a)
                s_next, rewards, dones = env.step(actions)
                eff_dt = float(env.last_step_ratio) * float(env.dt)
                t_acc += eff_dt
                for i in range(2):
                    ep_buf.append({
                        "s": states[i],
                        "r": rewards[i],
                        "s_next": s_next[i],
                        "done": (dones[i] != 0),
                        "dt": eff_dt
                    })
                # è®°å½•è½¨è¿¹æ•°æ®ï¼ˆagent0è§†è§’ï¼‰
                ep_times.append(t_acc)
                ep_positions.append([
                    [s_next[0].self_state.px, s_next[0].self_state.py],
                    [s_next[0].neighbor_state.px, s_next[0].neighbor_state.py]
                ])

                ep_reward += rewards[0]
                states = s_next
                done = [d != 0 for d in dones]
                steps += 1

            # episode ç»“æŸåç»Ÿä¸€å†™å…¥
            for tr in ep_buf:
                replay.push(**tr)
            episode_rewards.append(ep_reward)
            episode_lengths.append(steps)

            # æ„é€ å®Œæ•´Trajectoryå¯¹è±¡å¹¶ç¼“å­˜
            if len(ep_positions) >= 2:
                traj = Trajectory(
                    gamma=float(cfg.model.gamma),
                    goal_x=float(cfg.sim.crossing_radius),
                    goal_y=0.0,
                    radius=float(cfg.agent.radius),
                    v_pref=float(cfg.agent.v_pref),
                    times=np.array(ep_times, dtype=np.float32),
                    positions=np.array(ep_positions, dtype=np.float32),
                    kinematic=bool(cfg.agent.kinematic)
                )
                recent_episodes.append(traj)
                if len(recent_episodes) > 5:
                    recent_episodes.pop(0)

                # æ¯ä¸ªepisodeåéƒ½åšä¸€æ¬¡FQIå¾®è°ƒï¼ˆåªè¦recent_episodesæ»¡5æ¡ï¼‰
                if len(recent_episodes) == 5:
                    tdgen = TDTarget(gamma=float(cfg.model.gamma), model_or_fn=model, device=device)
                    all_pairs = []
                    for t in recent_episodes:
                        pairs = tdgen.compute_pairs(t, times=t.times.tolist())
                        all_pairs.extend(pairs)
                    if all_pairs:
                        s_batch = torch.stack([p[0].flatten() for p in all_pairs]).to(device)
                        y_batch = torch.stack([p[1].flatten() for p in all_pairs]).to(device)
                        model.train()
                        optimizer.zero_grad()
                        pred = model(s_batch)
                        loss_fqi = nn.functional.mse_loss(pred, y_batch)
                        loss_fqi.backward()
                        optimizer.step()
                        tqdm.write(f"ğŸ’¡ FQIå¾®è°ƒ: ç”¨{len(all_pairs)}å¯¹æ ·æœ¬åšä¸€è½®å›å½’, loss={loss_fqi.item():.4f}")


        # learning step if enough data
        train_loss = 0.0
        if len(replay) >= max(1, batch_size):
            batch = replay.sample(batch_size, generator=generator)
            s = batch["s"].to(device)
            r = batch["r"].to(device)
            s_next = batch["s_next"].to(device)
            done = batch["done"].to(device)
            dt_b = batch["dt"].to(device)

            with torch.no_grad():
                v_next = model(s_next).detach()
            v = model(s)

            gamma = float(cfg.model.gamma)
            v_pref = float(cfg.agent.v_pref)
            g = gamma ** (dt_b * v_pref)
            y = r + g * v_next * (~done)

            loss = nn.functional.mse_loss(v, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss = loss.item()

        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
        avg_reward = np.mean(episode_rewards) if episode_rewards else 0.0
        avg_length = np.mean(episode_lengths) if episode_lengths else 0.0
        train_pbar.set_postfix({
            "loss": f"{train_loss:.4f}",
            "reward": f"{avg_reward:.2f}",
            "steps": f"{avg_length:.1f}",
            "replay": len(replay)
        })

        # checkpoint regularly
        if (epoch + 1) % int(cfg.train.checkpoint_interval) == 0:
            ckpt_path = os.path.join(base_dir, f"checkpoint_epoch_{epoch+1}.pt")
            _save_checkpoint(model, ckpt_path)
            tqdm.write(f"ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {ckpt_path}")

    train_pbar.close()

    # final save
    final_path = os.path.join(base_dir, "final.pt")
    _save_checkpoint(model, final_path)

    print("\n" + "="*70)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print(f"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_path}")
    print(f"âœ“ æ£€æŸ¥ç‚¹ç›®å½•: {base_dir}")
    print(f"âœ“ è®­ç»ƒæ€»è½®æ•°: {num_epochs}")
    print(f"âœ“ ç»éªŒæ± å¤§å°: {len(replay)}")
    print("="*70 + "\n")


__all__ = ["run_training"]
