"""
Minimal training loop for CADRL.

This module exposes `run_training(cfg, ...)` which can be imported and called
from `main.py` (CLI handling should live in main.py as requested).

The function implements a simple TD(0) training loop using the project"s
`CADRLEnv`, `ActionSpace`, `ValueNetwork`, `ReplayMemory`, and `CADRLAgent`.
"""
from typing import Optional, Dict, List
import os
import time

import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm

from core import ConfigContainer, get_config_container
from env import CADRLEnv
from agent import CADRLAgent
from models import ValueNetwork
from utils import ActionSpace, ReplayMemory
from utils.trajectory import Trajectory
from utils.value_target import ArrivalTimeTarget


def load_trajectories(path: str, *, skip_timed_out: bool = True) -> List[Trajectory]:
    """
    Load trajectories from `path` and return a list of `Trajectory` objects.

    NOTE: placeholder. Implement data loading in your environment and
    return a List[Trajectory].
    """
    trajs: List[Trajectory] = []
    if not os.path.exists(path):
        return trajs

    files = sorted([f for f in os.listdir(path) if f.endswith(".npz")])
    for fn in files:
        fp = os.path.join(path, fn)
        try:
            with np.load(fp) as data:
                # skip trajectories that were timed out or went out of bounds (generator may add these fields)
                if skip_timed_out and (bool(data.get("timed_out", False)) or bool(data.get("out_of_bounds", False))):
                    continue
                times = data["times"]
                positions = data["positions"]  # (T,2,2)
                v_pref = float(data.get("v_pref", 1.0))
                radius = float(data.get("radius", 0.3))
                goal_x = float(data.get("goal_x", 0.0))
                goal_y = float(data.get("goal_y", 0.0))
                kinematic = bool(data.get("kinematic", True))

                # build Trajectory: requires gamma, goal_x, goal_y, radius, v_pref, times, positions
                traj = Trajectory(
                    gamma=float(getattr(get_config_container().model, "gamma", 0.8)),
                    goal_x=float(goal_x),
                    goal_y=float(goal_y),
                    radius=float(radius),
                    v_pref=float(v_pref),
                    times=np.asarray(times, dtype=np.float32),
                    positions=np.asarray(positions, dtype=np.float32),
                    kinematic=bool(kinematic),
                )
                trajs.append(traj)
        except Exception: # pylint: disable=broad-except
            # skip malformed files
            continue
    return trajs


def _make_dirs(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def _save_checkpoint(model: nn.Module, path: str) -> None:
    torch.save({"state_dict": model.state_dict()}, path)


def run_training(
    cfg: Optional[Dict] = None,
    *,
    device: Optional[torch.device] = None,
    resume: Optional[str] = None,
    run_name: Optional[str] = None,
) -> None:
    """
    Run a minimal TD training loop.

    Parameters
    ----------
    cfg : Optional[Dict]
        If None, load configuration via `get_config_container()`; otherwise
        expects a ConfigContainer or dict-like mapping with required fields.
    device : Optional[torch.device]
        Device to run training on; if None, use cfg.device.
    resume : Optional[str]
        Path to a checkpoint to resume from (optional).
    run_name : Optional[str]
        Name for this run; used to create checkpoint/log directories.

    Notes
    -----
    This is intentionally minimal: it demonstrates integration with the
    repository"s components. It is not optimized for speed or stability.
    """
    # load config container if needed
    if cfg is None:
        cfg = get_config_container()
    # allow passing a raw dict -> get a ConfigContainer
    if isinstance(cfg, dict):
        cfg = ConfigContainer.from_dict(cfg)
    if not hasattr(cfg, "agent"):
        # fallback to loader
        cfg = get_config_container()

    device = device or getattr(cfg, "device", torch.device("cpu"))

    # build paths
    base_dir = os.path.join("checkpoints", run_name or time.strftime("%Y%m%d-%H%M%S"))
    _make_dirs(base_dir)

    # environment and action space
    env = CADRLEnv(cfg, phase="train")
    action_space = ActionSpace(cfg.agent.v_pref, cfg.agent.kinematic)

    # model and replay
    model = ValueNetwork(state_dim=cfg.model.state_dim, device=device).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=float(cfg.train.learning_rate))
    replay = ReplayMemory(capacity=int(cfg.train.capacity), state_dim=int(cfg.model.state_dim))

    # -------------------- supervised pretraining (optional) --------------------

    pretrain_epochs = getattr(cfg.init, "pretrain_epochs", 0)
    pretrain_batch_size = getattr(cfg.init, "pretrain_batch_size", 500)
    pretrain_total_iters = getattr(cfg.init, "pretrain_total_iters", 10000)
    traj_dir = getattr(cfg.init, "traj_dir", None)
    if pretrain_epochs > 0 and traj_dir:
        print("\n" + "="*70)
        print("ğŸš€ å¼€å§‹é¢„è®­ç»ƒé˜¶æ®µ (Supervised Pretraining)")
        print("="*70)
        try:
            trajectories = load_trajectories(traj_dir)
        except NotImplementedError:
            trajectories = []

        if trajectories:
            print(f"âœ“ åŠ è½½äº† {len(trajectories)} æ¡è½¨è¿¹æ•°æ®")
            pair_gen = ArrivalTimeTarget(gamma=float(cfg.model.gamma))
            # simple pretraining loop (MSE on arrival time targets)
            preoptimizer = torch.optim.Adam(model.parameters(), lr=float(cfg.train.learning_rate))
            model.train()

            # é¢„è®­ç»ƒè¿›åº¦æ¡
            # å…¼å®¹åŸè®ºæ–‡ï¼šæ€»å…± pretrain_total_iters æ¬¡ï¼Œæ¯æ¬¡ batch_size
            pairs = []
            for traj in trajectories:
                pairs.extend(pair_gen.compute_pairs(traj))
            total_pairs = len(pairs)
            pretrain_pbar = tqdm(range(pretrain_total_iters), desc="é¢„è®­ç»ƒè¿›åº¦",
                                unit="iter", colour="blue", ncols=100)
            for _ in pretrain_pbar:
                # éšæœºé‡‡æ · batch
                idx = torch.randperm(total_pairs)[:pretrain_batch_size]
                batch_pairs = [pairs[i] for i in idx]
                s_batch = torch.stack([p[0].flatten() for p in batch_pairs]).to(device)
                y_batch = torch.stack([p[1].flatten() for p in batch_pairs]).to(device)
                pred = model(s_batch)
                loss_pre = nn.functional.mse_loss(pred, y_batch)
                preoptimizer.zero_grad()
                loss_pre.backward()
                preoptimizer.step()
                pretrain_pbar.set_postfix({
                    "loss": f"{loss_pre.item():.6f}",
                    "pairs": total_pairs,
                    "batch": pretrain_batch_size
                })
            pretrain_pbar.close()
            print(f"âœ“ é¢„è®­ç»ƒå®Œæˆï¼å…±è®­ç»ƒ {pretrain_total_iters} æ¬¡ batch")
            # end pretraining
        else:
            print("âš  è­¦å‘Šï¼šæœªæ‰¾åˆ°æœ‰æ•ˆçš„è½¨è¿¹æ•°æ®ï¼Œè·³è¿‡é¢„è®­ç»ƒ")
    else:
        if pretrain_epochs == 0:
            print("\nğŸ“ é¢„è®­ç»ƒå·²ç¦ç”¨ (pretrain_epochs=0)")
        else:
            print("\nâš  è­¦å‘Šï¼šæœªé…ç½®è½¨è¿¹ç›®å½•ï¼Œè·³è¿‡é¢„è®­ç»ƒ")

    # agent that uses the value network for action selection
    agent = CADRLAgent(value_net=model, action_space=action_space, device=device, gamma=float(cfg.model.gamma))

    # optionally resume
    if resume is not None and os.path.exists(resume):
        ckpt = torch.load(resume, map_location=device)
        try:
            model.load_state_dict(ckpt.get("state_dict", ckpt))
        except Exception: # pylint: disable=broad-except
            model.load_state_dict(ckpt)

    # training loop (minimal)
    num_epochs = int(cfg.train.num_epochs)
    sample_episodes = int(cfg.train.sample_episodes)
    batch_size = int(cfg.train.batch_size)

    generator = torch.Generator()

    print("\n" + "="*70)
    print("ğŸ¯ å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒé˜¶æ®µ (Reinforcement Learning)")
    print("="*70)
    print("é…ç½®ä¿¡æ¯:")
    print(f"  - è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"  - æ¯è½®é‡‡æ ·episodes: {sample_episodes}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  - å­¦ä¹ ç‡: {cfg.train.learning_rate}")
    print(f"  - Gamma: {cfg.model.gamma}")
    print(f"  - è®¾å¤‡: {device}")
    print(f"  - æ£€æŸ¥ç‚¹é—´éš”: {cfg.train.checkpoint_interval} epochs")
    print("="*70 + "\n")

    # è®­ç»ƒä¸»å¾ªç¯è¿›åº¦æ¡
    train_pbar = tqdm(range(num_epochs), desc="è®­ç»ƒè¿›åº¦",
                     unit="epoch", colour="green", ncols=100)

    # è¯»å–æ‰°åŠ¨è°ƒåº¦é…ç½®
    disturb_schedule = getattr(cfg.train, "disturb_cr_schedule", None)
    if disturb_schedule is None:
        # é»˜è®¤é…ç½®ï¼ˆå…¨ç¨‹å›ºå®šæ‰°åŠ¨ï¼‰
        disturb_schedule = {
            "stage1_epochs": 0,
            "stage1_range": [1.0, 1.0],
            "stage2_epochs": 0,
            "stage2_range": [0.9, 1.1],
            "stage3_epochs": 0,
            "stage3_range": [0.8, 1.2],
            "stage4_range": [0.7, 1.3]
        }

    recent_episodes = []  # ç¼“å­˜æœ€è¿‘5æ¡episodeçš„Trajectoryå¯¹è±¡

    for epoch in train_pbar:
        # åŠ¨æ€è°ƒæ•´æ‰°åŠ¨èŒƒå›´ï¼ˆåˆ†é˜¶æ®µæ¸è¿›ï¼‰
        stage1 = disturb_schedule.get("stage1_epochs", 0)
        stage2 = disturb_schedule.get("stage2_epochs", 0)
        stage3 = disturb_schedule.get("stage3_epochs", 0)

        if epoch < stage1:
            env.disturb_range = tuple(disturb_schedule.get("stage1_range", [1.0, 1.0]))
        elif epoch < stage1 + stage2:
            env.disturb_range = tuple(disturb_schedule.get("stage2_range", [0.9, 1.1]))
        elif epoch < stage1 + stage2 + stage3:
            env.disturb_range = tuple(disturb_schedule.get("stage3_range", [0.8, 1.2]))
        else:
            env.disturb_range = tuple(disturb_schedule.get("stage4_range", [0.7, 1.3]))
        # collect some episodes into replay
        episode_rewards = []
        episode_lengths = []
        success_count = 0  # ç»Ÿè®¡æœ¬è½®æˆåŠŸåˆ°è¾¾çš„ episode æ•°

        for _ in range(sample_episodes):
            states = env.reset()
            done = [False, False]
            steps = 0
            ep_reward = 0.0
            ep_buf = []

            # è®°å½•å®Œæ•´è½¨è¿¹ä¿¡æ¯ï¼ˆç”¨äºæ„é€ Trajectoryï¼‰
            # ä¿®æ­£ï¼šç¼“å­˜åˆå§‹ç›®æ ‡åæ ‡ï¼ˆä»åˆå§‹çŠ¶æ€è¯»å–ï¼‰
            initial_goal_x = float(states[0].self_state.pgx)
            initial_goal_y = float(states[0].self_state.pgy)

            ep_times = [0.0]
            ep_positions = [[
                [states[0].self_state.px, states[0].self_state.py],
                [states[0].neighbor_state.px, states[0].neighbor_state.py]
            ]]
            t_acc = 0.0


            def _eps(epoch, total, eps_start=0.2, eps_end=0.02):
                t = min(1.0, epoch / max(1, total))
                return eps_start + (eps_end - eps_start) * t

            while not all(done) and steps < env.max_steps:
                actions = []
                for i in range(2):
                    s = states[i]
                    if torch.rand(()) < _eps(epoch, num_epochs):
                        a = action_space.sample()
                    else:
                        a = agent.act(s, env, i, mode="train")
                    actions.append(a)
                s_next, rewards, dones = env.step(actions)
                # ä¿®æ­£ï¼šeff_dtåŠ ä¸‹ç•Œä¿æŠ¤ï¼Œé¿å…æŠ˜æ‰£å¤±æ•ˆ
                eff_dt = max(1e-3, float(env.last_step_ratio) * float(env.dt))
                t_acc += eff_dt
                # å†™å…¥ç»éªŒå›æ”¾ï¼ˆä¸¤åæ™ºèƒ½ä½“éƒ½å†™å…¥ï¼Œä¿ç•™JointStateï¼‰
                for i in range(2):
                    replay.push(
                        states[i],
                        float(rewards[i]),
                        s_next[i],
                        bool(dones[i] != 0),
                        float(eff_dt),
                    )
                for i in range(2):
                    ep_buf.append({
                        "s": states[i],
                        "r": rewards[i],
                        "s_next": s_next[i],
                        "done": (dones[i] != 0),
                        "dt": eff_dt
                    })
                # è®°å½•è½¨è¿¹æ•°æ®ï¼ˆagent0è§†è§’ï¼‰
                ep_times.append(t_acc)
                ep_positions.append([
                    [s_next[0].self_state.px, s_next[0].self_state.py],
                    [s_next[0].neighbor_state.px, s_next[0].neighbor_state.py]
                ])

                # ä¿®æ­£ï¼šå…ˆç´¯è®¡å½“å‰æ­¥å¥–åŠ±ï¼ˆåŒ…æ‹¬ç»ˆæ­¢æ­¥ï¼‰ï¼Œå†æ›´æ–° done çŠ¶æ€
                # è¿™æ ·ç»ˆæ­¢æ­¥çš„ goal_reward æˆ– collision_penalty æ‰ä¼šè¢«æ­£ç¡®è®¡å…¥
                ep_reward += rewards[0]
                
                states = s_next
                done = [d != 0 for d in dones]
                steps += 1
            reward_types = {"living_cost": 0, "near": 0, "collision": 0, "goal": 0, "zero": 0, "other": 0}
            other_samples = []  # ç”¨äºè°ƒè¯•ï¼šè®°å½•å‰å‡ ä¸ª "other" ç±»çš„å¥–åŠ±å€¼
            for idx, tr in enumerate(ep_buf):
                r = tr["r"]
                # ç²¾ç¡®åˆ¤æ–­rewardç±»å‹
                if abs(r) < 1e-9:
                    # å·²ç»ˆæ­¢æ™ºèƒ½ä½“çš„åç»­æ­¥éª¤ï¼ˆreward=0ï¼‰
                    reward_types["zero"] += 1
                elif abs(r - (-0.008 * float(tr["dt"])) ) < 1e-5:
                    reward_types["living_cost"] += 1
                elif abs(r + 0.25) < 1e-4:
                    reward_types["collision"] += 1
                # ç»ˆæ­¢æ­¥æœ‰å¾®å°çš„æ—¶é—´æƒ©ç½šï¼ˆä¾‹å¦‚ 1.0 - 0.008*dt â‰ˆ 0.9992ï¼‰ï¼Œæ”¾å®½é˜ˆå€¼
                elif (r > 0.9) or (abs(r - 1.0) < 2e-3):
                    reward_types["goal"] += 1
                elif r < -0.01 and r > -0.25:
                    reward_types["near"] += 1
                else:
                    reward_types["other"] += 1
                    if len(other_samples) < 5:  # åªè®°å½•å‰5ä¸ª
                        other_samples.append(f"{r:.6f}")
            
            # åˆ¤æ–­æ˜¯å¦æˆåŠŸåˆ°è¾¾ï¼ˆagent 0 çš„ done ä¸º 1ï¼‰
            if dones[0] == 1:
                success_count += 1
            
            # æ·»åŠ æ›´è¯¦ç»†çš„ç»ˆæ­¢çŠ¶æ€ä¿¡æ¯
            done_status = {0: "æ´»è·ƒ", 1: "åˆ°è¾¾âœ…", 2: "ç¢°æ’âŒ", 3: "è¶Šç•ŒâŒ", 4: "è¶…æ—¶âŒ"}
            agent0_status = done_status.get(dones[0], f"æœªçŸ¥({dones[0]})")
            agent1_status = done_status.get(dones[1], f"æœªçŸ¥({dones[1]})")
            
            other_info = f" other_samples={other_samples}" if other_samples else ""
            # print(f"[A0_reward={ep_reward:.3f}, steps={steps}, A0:{agent0_status}, A1:{agent1_status}] rewardåˆ†å¸ƒ: {reward_types}{other_info}", flush=True)

            episode_rewards.append(ep_reward)
            episode_lengths.append(steps)

            # æ„é€ å®Œæ•´Trajectoryå¯¹è±¡å¹¶ç¼“å­˜
            if len(ep_positions) >= 2:
                traj = Trajectory(
                    gamma=float(cfg.model.gamma),
                    goal_x=initial_goal_x,
                    goal_y=initial_goal_y,
                    radius=float(cfg.agent.radius),
                    v_pref=float(cfg.agent.v_pref),
                    times=np.array(ep_times, dtype=np.float32),
                    positions=np.array(ep_positions, dtype=np.float32),
                    kinematic=bool(cfg.agent.kinematic)
                )
                recent_episodes.append(traj)
                if len(recent_episodes) > 5:
                    recent_episodes.pop(0)

                # ===== FQIå¾®è°ƒå·²æš‚æ—¶ç¦ç”¨ =====
                # åŸå› ï¼šé¿å…valueçˆ†ç‚¸æ—¶å¾®è°ƒè¿›ä¸€æ­¥æ¶åŒ–
                # å¦‚éœ€å¯ç”¨ï¼Œå–æ¶ˆä¸‹é¢æ³¨é‡Šå³å¯
                # if len(recent_episodes) == 5:
                #     tdgen = TDTarget(gamma=float(cfg.model.gamma), model_or_fn=model, device=device)
                #     all_pairs = []
                #     for t in recent_episodes:
                #         pairs = tdgen.compute_pairs(t, times=t.times.tolist())
                #         all_pairs.extend(pairs)
                #     if all_pairs:
                #         s_batch = torch.stack([p[0].flatten() for p in all_pairs]).to(device)
                #         y_batch = torch.stack([p[1].flatten() for p in all_pairs]).to(device)
                #         model.train()
                #         optimizer.zero_grad()
                #         pred = model(s_batch)
                #         loss_fqi = nn.functional.mse_loss(pred, y_batch)
                #         loss_fqi.backward()
                #         optimizer.step()
                #         tqdm.write(f"ğŸ’¡ FQIå¾®è°ƒ: ç”¨{len(all_pairs)}å¯¹æ ·æœ¬åšä¸€è½®å›å½’, loss={loss_fqi.item():.8f}")


        # learning step if enough data
        train_loss = 0.0
        if len(replay) >= max(1, batch_size):
            batch = replay.sample(batch_size, generator=generator)
            s = batch["s"].to(device)
            r = batch["r"].to(device)
            s_next = batch["s_next"].to(device)
            done = batch["done"].to(device)
            dt_b = batch["dt"].to(device)

            with torch.no_grad():
                v_next = model(s_next).detach()
                # Clamp predicted next-state values to a safe range to avoid exploding targets
                v_next = torch.clamp(v_next, -2.0, 2.0)
            v = model(s)

            gamma = float(cfg.model.gamma)
            dt_ref = float(cfg.sim.dt)  # å‚è€ƒæ—¶é—´æ­¥é•¿
            # ä¿®æ­£ï¼šä½¿ç”¨æ—¶é—´æŠ˜æ‰£ï¼Œå»æ‰v_pref
            g = gamma ** (dt_b / dt_ref)
            # ä¿®æ­£ï¼šdoneæ©ç æ˜¾å¼è½¬æ¢
            done_mask = (~done.bool()).float()
            y = r + g * v_next * done_mask
            # Final safeguard: clamp TD target
            y = torch.clamp(y, -2.0, 2.0)

            # å®‰å…¨æ£€æŸ¥ï¼šTD targetæ˜¯å¦åˆç†
            if torch.any(y < -2.0) or torch.any(y > 2.0):
                bad_indices = torch.where((y < -2.0) | (y > 2.0))[0]
                tqdm.write("\n" + "="*70)
                tqdm.write("âš ï¸ TD TARGET OUT OF RANGE âš ï¸")
                tqdm.write("="*70)
                for idx in bad_indices[:5]:  # åªæ‰“å°å‰5ä¸ª
                    tqdm.write(f"Sample {idx}: y={y[idx].item():.6f}, r={r[idx].item():.6f}, "
                              f"v_next={v_next[idx].item():.6f}, g={g[idx].item():.6f}, "
                              f"done_mask={done_mask[idx].item():.1f}")
                tqdm.write("="*70 + "\n")

            loss = nn.functional.mse_loss(v, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss = loss.item()

        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
        avg_reward = np.mean(episode_rewards) if episode_rewards else 0.0
        avg_length = np.mean(episode_lengths) if episode_lengths else 0.0
        success_rate = success_count / max(1, sample_episodes)
        train_pbar.set_postfix({
            "loss": f"{train_loss:.6f}",
            "reward": f"{avg_reward:.2f}",
            "succ": f"{success_rate:.2f}",
            "replay": f"{len(replay)}",
            "steps": f"{avg_length:.1f}"
        })

        # checkpoint regularly
        if (epoch + 1) % int(cfg.train.checkpoint_interval) == 0:
            ckpt_path = os.path.join(base_dir, f"checkpoint_epoch_{epoch+1}.pt")
            _save_checkpoint(model, ckpt_path)
            tqdm.write(f"ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {ckpt_path}")

    train_pbar.close()

    # final save
    final_path = os.path.join(base_dir, "final.pt")
    _save_checkpoint(model, final_path)

    print("\n" + "="*70)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print(f"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_path}")
    print(f"âœ“ æ£€æŸ¥ç‚¹ç›®å½•: {base_dir}")
    print(f"âœ“ è®­ç»ƒæ€»è½®æ•°: {num_epochs}")
    print(f"âœ“ ç»éªŒæ± å¤§å°: {len(replay)}")
    print("="*70 + "\n")


__all__ = ["run_training"]
