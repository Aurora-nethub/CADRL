# -*- coding: utf-8 -*-
"""
è„šæœ¬ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ rollout ä¸€æ¡æ–°è½¨è¿¹å¹¶å¯è§†åŒ–
"""
from typing import Optional
import os
import random
import torch
import numpy as np
from core import get_config_container, ConfigContainer
from models import ValueNetwork
from agent import CADRLAgent
from env import CADRLEnv
from utils import ActionSpace
from utils.trajectory import Trajectory
from vis.visualize import plot_trajectory

def rollout_episode(env, agent, max_steps=200):
    states = env.reset()
    done = [False, False]
    steps = 0

    # è®°å½•å®é™…çš„ç›®æ ‡ï¼ˆè€ƒè™‘cræ‰°åŠ¨ï¼‰
    actual_goal_x = states[0].self_state.pgx
    actual_goal_y = states[0].self_state.pgy
    # Agent 1ï¼ˆé‚»å±…ï¼‰çš„ç›®æ ‡
    neighbor_goal_x = states[1].self_state.pgx
    neighbor_goal_y = states[1].self_state.pgy

    ep_times = [0.0]
    ep_positions = [[
        [states[0].self_state.px, states[0].self_state.py],
        [states[0].neighbor_state.px, states[0].neighbor_state.py]
    ]]
    t_acc = 0.0
    while not all(done) and steps < max_steps:
        actions = []
        for i in range(2):
            s = states[i]
            a = agent.act(s, env, i, mode='eval')
            actions.append(a)
        s_next, _, dones = env.step(actions)
        eff_dt = float(env.last_step_ratio) * float(env.dt)
        t_acc += eff_dt
        ep_times.append(t_acc)
        ep_positions.append([
            [s_next[0].self_state.px, s_next[0].self_state.py],
            [s_next[0].neighbor_state.px, s_next[0].neighbor_state.py]
        ])
        states = s_next
        done = [d != 0 for d in dones]
        steps += 1
    traj = Trajectory(
        gamma=env.cfg.model.gamma,
        goal_x=actual_goal_x,  # ä½¿ç”¨å®é™…ç›®æ ‡ï¼ˆè€ƒè™‘æ‰°åŠ¨ï¼‰
        goal_y=actual_goal_y,
        radius=env.cfg.agent.radius,
        v_pref=env.cfg.agent.v_pref,
        times=np.array(ep_times, dtype=np.float32),
        positions=np.array(ep_positions, dtype=np.float32),
        kinematic=env.cfg.agent.kinematic
    )
    # é™„åŠ é‚»å±…ç›®æ ‡ä¿¡æ¯ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    traj.neighbor_goal_x = neighbor_goal_x
    traj.neighbor_goal_y = neighbor_goal_y
    return traj


def run_test(
    cfg: Optional[ConfigContainer] = None,
    *,
    device: Optional[torch.device] = None,
    run_name: Optional[str] = None,
    case: int = None
) -> None:
    """
    è¿è¡Œæµ‹è¯•ï¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œrolloutæ–°è½¨è¿¹å¹¶å¯è§†åŒ–
    
    Parameters
    ----------
    cfg : Optional[ConfigContainer]
        é…ç½®å®¹å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åŠ è½½
    device : Optional[torch.device]
        è¿è¡Œè®¾å¤‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
    run_name : Optional[str]
        è¿è¡Œåç§°ï¼Œç”¨äºå®šä½checkpointç›®å½•
    """
    n_cases = 20
    if case is None:
        case = random.randint(0, n_cases - 1)
    if cfg is None:
        cfg = get_config_container()
    if isinstance(cfg, dict):
        cfg = ConfigContainer.from_dict(cfg)

    device = device or getattr(cfg, 'device', torch.device('cpu'))

    # å®šä½checkpoint
    if run_name:
        ckpt_dir = os.path.join('checkpoints', run_name)
    else:
        # ä½¿ç”¨æœ€æ–°çš„checkpoint
        ckpt_base = 'checkpoints'
        if not os.path.exists(ckpt_base) or not os.listdir(ckpt_base):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°checkpointç›®å½•: {ckpt_base}")
        ckpt_dir = os.path.join(ckpt_base, sorted(os.listdir(ckpt_base))[-1])

    model_path = os.path.join(ckpt_dir, 'final.pt')
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")

    print(f"\n{'='*70}")
    print('ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å‹')
    print(f"{'='*70}")
    print(f'æ¨¡å‹è·¯å¾„: {model_path}')
    print(f'è®¾å¤‡: {device}')

    # åŠ è½½æ¨¡å‹ï¼ˆç›´æ¥åŠ è½½state_dictï¼Œä¸éœ€è¦configï¼‰
    ckpt = torch.load(model_path, map_location=device)
    model = ValueNetwork(state_dim=cfg.model.state_dim, device=device).to(device)
    model.load_state_dict(ckpt.get('state_dict', ckpt))
    model.eval()
    print('âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ')

    # æ„é€ agentå’Œç¯å¢ƒ
    action_space = ActionSpace(cfg.agent.v_pref, cfg.agent.kinematic)
    agent = CADRLAgent(value_net=model, action_space=action_space, device=device, gamma=cfg.model.gamma)

    # æµ‹è¯•æ—¶å…³é—­cræ‰°åŠ¨ï¼Œä¿è¯èµ·ç‚¹-ç›®æ ‡è·ç¦»ä¸€è‡´æ€§
    env = CADRLEnv(cfg, phase='test')
    env.disturb_range = (1.0, 1.0)  # æµ‹è¯•æ—¶å›ºå®šcrï¼Œé¿å…ç›®æ ‡ä¸åŒ¹é…

    # rolloutå‡½æ•°ï¼ˆæ”¯æŒæŒ‡å®šcaseï¼‰
    def rollout_episode_case(env, agent, max_steps=200, case=0):
        # ä¼ é€’caseå’Œè§’åº¦ï¼Œresetå†…éƒ¨è‡ªåŠ¨å¤„ç†
        states = env.reset(case=case)
        done = [False, False]
        steps = 0
        # Agent 0çš„ç›®æ ‡
        actual_goal_x = states[0].self_state.pgx
        actual_goal_y = states[0].self_state.pgy
        # Agent 1ï¼ˆé‚»å±…ï¼‰çš„ç›®æ ‡
        neighbor_goal_x = states[1].self_state.pgx
        neighbor_goal_y = states[1].self_state.pgy
        ep_times = [0.0]
        ep_positions = [[
            [states[0].self_state.px, states[0].self_state.py],
            [states[0].neighbor_state.px, states[0].neighbor_state.py]
        ]]
        t_acc = 0.0
        while not all(done) and steps < max_steps:
            actions = []
            for i in range(2):
                s = states[i]
                a = agent.act(s, env, i, mode='eval')
                actions.append(a)
            s_next, _, dones = env.step(actions)
            eff_dt = float(env.last_step_ratio) * float(env.dt)
            t_acc += eff_dt
            ep_times.append(t_acc)
            ep_positions.append([
                [s_next[0].self_state.px, s_next[0].self_state.py],
                [s_next[0].neighbor_state.px, s_next[0].neighbor_state.py]
            ])
            states = s_next
            done = [d != 0 for d in dones]
            steps += 1
        traj = Trajectory(
            gamma=env.cfg.model.gamma,
            goal_x=actual_goal_x,
            goal_y=actual_goal_y,
            radius=env.cfg.agent.radius,
            v_pref=env.cfg.agent.v_pref,
            times=np.array(ep_times, dtype=np.float32),
            positions=np.array(ep_positions, dtype=np.float32),
            kinematic=env.cfg.agent.kinematic
        )
        # é™„åŠ é‚»å±…ç›®æ ‡ä¿¡æ¯ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        traj.neighbor_goal_x = neighbor_goal_x
        traj.neighbor_goal_y = neighbor_goal_y
        # é™„åŠ doneåŸå› 
        traj.done_reason = dones
        return traj

    traj = rollout_episode_case(env, agent, max_steps=cfg.sim.max_time, case=case)
    print('âœ“ Rolloutå®Œæˆ')
    print(f'  - è½¨è¿¹é•¿åº¦: {len(traj)} æ­¥')
    print(f'  - è‡ªè½¦èµ·ç‚¹: ({traj.positions[0,0,0]:.2f}, {traj.positions[0,0,1]:.2f})')
    print(f'  - è‡ªè½¦ç›®æ ‡: ({traj.goal_x:.2f}, {traj.goal_y:.2f})')
    print(f'  - è‡ªè½¦ç»ˆç‚¹: ({traj.positions[-1,0,0]:.2f}, {traj.positions[-1,0,1]:.2f})')
    print(f'  - é‚»å±…èµ·ç‚¹: ({traj.positions[0,1,0]:.2f}, {traj.positions[0,1,1]:.2f})')
    print(f'  - é‚»å±…ç»ˆç‚¹: ({traj.positions[-1,1,0]:.2f}, {traj.positions[-1,1,1]:.2f})')
    
    # é™„åŠ doneåŸå› çš„è¾“å‡º
    if hasattr(traj, 'done_reason'):
        done_map = {0: 'Active', 1: 'Reached', 2: 'Collision', 3: 'Out-of-Bound', 4: 'Timeout'}
        print(f'  - DoneåŸå› : Agent0={done_map.get(traj.done_reason[0], "Unknown")}, Agent1={done_map.get(traj.done_reason[1], "Unknown")}')

    # å¯è§†åŒ–
    vis_dir = 'vis'
    os.makedirs(vis_dir, exist_ok=True)
    rname = run_name or 'test'
    save_path = os.path.join(vis_dir, f'rollout_{rname}.png')
    plot_trajectory(traj, save_path=save_path, value_fn=model)
    print(f'\nâœ“ å¯è§†åŒ–å·²ä¿å­˜è‡³: {save_path}')
    print(f"{'='*70}\n")


if __name__ == '__main__':
    run_test()
